{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import zipfile\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################33\n",
    "#### Card Dataset\n",
    "\n",
    "class CardDataset(Dataset):\n",
    "    \"\"\"NailDataset dataset.\n",
    "    img generator that take file names and selected poligons from csv_file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, augmentation=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "\n",
    "            nail_dataset[img_number_in_csv][1] -image\n",
    "            nail_dataset[img_number_in_csv][0] -corresponding mask image\n",
    "        \"\"\"\n",
    "        self.nail_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nail_frame.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_id=self.nail_frame.iloc[idx][\"filename\"]\n",
    "        #\"read image\"\n",
    "        img_name = os.path.join(self.root_dir,self.nail_frame.iloc[idx][\"filename\"])\n",
    "        img_name=img_name.split('.')[0]+'.png'\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # read mask\n",
    "        mask_name=os.path.join(self.root_dir,self.nail_frame.iloc[idx][\"mask\"])\n",
    "        mask = cv2.imread(mask_name,cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # required for albumentation\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask[mask == 0] = 0.0\n",
    "        mask[mask == 255] = 1.0\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=mask)\n",
    "            image = sample['image']\n",
    "            mask=sample['mask']\n",
    "            mask=mask.unsqueeze(0)\n",
    "\n",
    "        return image,mask\n",
    "\n",
    "train_transform=A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5,border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()])\n",
    "\n",
    "\n",
    "validation_transform =A.Compose([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()])\n",
    "\n",
    "\n",
    "############################################################################################################33\n",
    "#### Card Model\n",
    "\n",
    "class CardModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,arch,encoder_name,encoder_weights,learning_rate,frozen_encoder=True):\n",
    "        super().__init__()\n",
    "        self.lr=learning_rate\n",
    "        self.save_hyperparameters()\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            classes=1,\n",
    "            #activation='sigmoid'\n",
    "            )\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE,from_logits=True)\n",
    "\n",
    "        if frozen_encoder==True:\n",
    "            for child in self.model.encoder.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "\n",
    "        image=batch[0]\n",
    "        # assert image (batch_size,num_channels,height,width)\n",
    "        assert image.ndim == 4\n",
    "        # assert the image dim is suitable for Unet\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask=batch[1]\n",
    "        assert mask.ndim == 4\n",
    "        assert mask.max()<=1 and  mask.min()>=0 # check that the mask between [0,1] not in range [0,255]\n",
    "\n",
    "        # calculate loss\n",
    "        output_mask = self.forward(image)\n",
    "        loss=self.loss_fn(output_mask,mask)\n",
    "\n",
    "        pred_mask=(output_mask.sigmoid() >= 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "        # calculate logs\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
    "\n",
    "        train_batch_dictionary={\"tp\":tp,\n",
    "                          \"fp\":fp,\n",
    "                          \"fn\":fn,\n",
    "                          \"tn\":tn}\n",
    "\n",
    "        self.training_step_outputs.append(train_batch_dictionary)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # aggregate step metics\n",
    "        outputs=self.training_step_outputs\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\") # mean (per image)\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\") # sum tp,fp,fn,tn over all dataset\n",
    "\n",
    "        metrics = {\n",
    "            \"train_per_image_iou\": per_image_iou,\n",
    "        #    \"train_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "\n",
    "        image=batch[0]\n",
    "        # assert image (batch_size,num_channels,height,width)\n",
    "        assert image.ndim == 4\n",
    "        # assert the image dim is suitable for Unet\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask=batch[1]\n",
    "        assert mask.ndim == 4\n",
    "        assert mask.max()<=1 and  mask.min()>=0 # check that the mask between [0,1] not in range [0,255]\n",
    "\n",
    "        # calculate loss\n",
    "        output_mask = self.forward(image)\n",
    "        loss=self.loss_fn(output_mask,mask)\n",
    "\n",
    "        pred_mask=(output_mask.sigmoid() >= 0.5).float()\n",
    "\n",
    "\n",
    "        # calculate logs\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
    "\n",
    "        validation_batch_dictionary={\"loss\":loss,\n",
    "                          \"tp\":tp,\n",
    "                          \"fp\":fp,\n",
    "                          \"fn\":fn,\n",
    "                          \"tn\":tn}\n",
    "\n",
    "        self.validation_step_outputs.append(validation_batch_dictionary)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # aggregate step metics\n",
    "        outputs= self.validation_step_outputs\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\") # mean (per image)\n",
    "        #dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\") # sum tp,fp,fn,tn over all dataset\n",
    "\n",
    "        metrics = {\n",
    "            \"validation_per_image_iou\": per_image_iou,\n",
    "            #\"validation_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=(self.lr or self.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(config=None):\n",
    "\n",
    "    # Initialize a WandB Run\n",
    "    wandb.init(config=config)\n",
    "\n",
    "    # Get hyperparameters from the run configs\n",
    "    config = wandb.config\n",
    "\n",
    "    N_EPOCH=20\n",
    "    ENCODER = 'resnet18'\n",
    "    Frozen_Encoder_Flag=True\n",
    "\n",
    "    data_path='/home/tatyana/Work_Invonto/Dataset/Card_Dataset/Dataset_ready_July2023'\n",
    "\n",
    "    ### Load Data\n",
    "    n_cpu = os.cpu_count()\n",
    "    #train\n",
    "    x_train_path=os.path.join(data_path,'Train')\n",
    "    #y_train_dataloader=os.path.join(x_train_path,'Train_DataLoader.csv')\n",
    "    y_train_dataloader=os.path.join(x_train_path,'Train_DataLoader_sample96.csv')\n",
    "    train_dataset = CardDataset(y_train_dataloader, x_train_path,transform=train_transform, augmentation=None)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,drop_last=True,num_workers=n_cpu)\n",
    "    #print('Number of training samples:', len(train_dataset))\n",
    "\n",
    "    # validation\n",
    "    x_valid_path = os.path.join(data_path, 'Validation')\n",
    "    y_valid_dataloader = os.path.join(x_valid_path, 'Validation_DataLoader.csv')\n",
    "    valid_dataset = CardDataset(y_valid_dataloader,x_valid_path,transform=validation_transform, augmentation=None)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False,num_workers=n_cpu)\n",
    "    #print('Number of validation samples:', len(valid_dataset))\n",
    "\n",
    "    \n",
    "    ### Define Model\n",
    "    card_model = CardModel(\"Unet\", ENCODER ,\"imagenet\",config.learning_rate,frozen_encoder=Frozen_Encoder_Flag)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"Card_detection\")\n",
    "    trainer = pl.Trainer(max_epochs=N_EPOCH,\n",
    "                     logger=wandb_logger,\n",
    "                     log_every_n_steps=1,\n",
    "                     default_root_dir=\"/content/drive/MyDrive/Dataset/\")\n",
    "    trainer.fit(\n",
    "        model=card_model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=valid_loader)\n",
    "    \n",
    "    # name=\"CardModel_\"+run.id\n",
    "    # path = \"/content/drive/MyDrive/Dataset/\"\n",
    "    # model_name=path+name+'.onnx'\n",
    "    # torch.onnx.export(card_model,\n",
    "    #               torch.randn(1,3,480,320).to('cpu'),\n",
    "    #               model_name,\n",
    "    #               input_names = ['input'],              # the model's input names\n",
    "    #               output_names = ['output'])\n",
    "    # wandb.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\":{\"name\":\"validation_per_image_iou\",\"goal\":\"maximize\"},\n",
    "    \"parameters\":{\n",
    "        'batch_size': {'values': [8,16,32]}, #256, 128, 64, \n",
    "        'learning_rate': {'values': [1e-1,1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Card_detection\")\n",
    "wandb.agent(sweep_id, train_run, count=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invonto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
